{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to prepare all data for the experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Python 3.8 or newer\n",
    "- pip\n",
    "\n",
    "In order to obtain a larger sample of papers on which logistic regression can be performed, it is adviced to obtain a **Semantic Scholar API key**, like mentioned in the original codebase. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arXiv Dataset\n",
    "\n",
    "Download the original dataset from the public ArXiv Dataset available on [Kaggle](https://www.kaggle.com/datasets/Cornell-University/arxiv/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Convert the dataset acquired as JSON into CSV for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python import_metadata.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Generate `categories.csv` for the next script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_categories.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "Select only papers in Computer Science category, and split dataset in train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python explore_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "Acquire additional details for each paper in the test set, necessary for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python get_paper_details.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "*(Optional: sample dataset)* \n",
    "Sample the test (and/or train) set to the desired size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python sample_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "Acquire additional details for all authors, external papers that cited a paper of the dataset, and all papers that a paper in the dataset cites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python get_authors_papers.py\n",
    "!python get_paper_citations.py\n",
    "!python get_references.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "Generate TF-IDF embeddings for all papers of the train and test set, and all authors that appear in both based on cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tfidf_authors.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8\n",
    "Randomly sample 1,000 authors for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python sample_random_authors.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9\n",
    "Generate a single data file containing the embeddings of all authors and all papers of the test set. This is required input for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate_data_source_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10\n",
    "Perform logistic regression on arXiv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python logistic_regression.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Books Reviews dataset\n",
    "The dataset for the extension was sourced from Kaggle, available on [Kaggle](https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1\n",
    "Split dataset in train and test set, create embeddings for all books, create embeddings for 1,000 random authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ./Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python amazon_exp_preparation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Generate a single data file containing the embeddings of all authors and all papers of the test set. This is required input for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../generate_data_source_file.py --df ./Amazon/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "Perform logistic regression on Amazon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python amazon_logreg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arXiv Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Perform experiment 1, recreating the unused figure in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./experiments/fig1.py --n 20 --m 40 --curves 10 --curve_pts 50 --ff ./experiments/fig1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Perform experiment 2a, recreating figure 1a in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./experiments/fig2a.py --n 20 --m 40 --curves 10 --curve_pts 50 --clusters 25 --components 2 --df ./Data/data_source_file_experiments.pickle --ff ./experiments/fig2a.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "Perform experiment 2b, recreating figure 2b in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./experiments/fig2b.py --n 20 --m 40 --curves 10 --curve_pts 50 --beta 0.9 --df ./Data/data_source_file_experiments.pickle --ff ./experiments/fig2b.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Books Reviews dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Perform experiment 2a for the Amazon Books Reviews dataset, recreating figure 1a in the original paper with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./experiments/fig2a.py --n 20 --m 40 --curves 10 --curve_pts 50 --clusters 25 --components 2 --df ./Amazon/Data/data_source_file_experiments.pickle --ff ./experiments/fig2a_amazon.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Perform experiment 2b for the Amazon Books Reviews dataset, recreating figure 1b in the original paper with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./experiments/fig2b.py --n 20 --m 40 --curves 10 --curve_pts 50 --beta 0.9 --df ./Amazon/Data/data_source_file_experiments.pickle --ff ./experiments/fig2b_amazon.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple recommendations per author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Perform experiment 2a with recommending k=3 papers per author, recreating figure 1a in the original paper with different k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./experiments/fig2a.py --n 20 --m 40 --curves 10 --curve_pts 50 --clusters 25 --components 2 --k 3 --df ./Data/data_source_file_experiments.pickle --ff ./experiments/fig2a_k3.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Perform experiment 2b with recommending k=3 papers per author, recreating figure 1b in the original paper with different k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./experiments/fig2b.py --n 20 --m 40 --curves 10 --curve_pts 50 --beta 0.9 --k 3 --df ./Data/data_source_file_experiments.pickle --ff ./experiments/fig2b_k3.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "Perform experiment 2a with recommending k=5 papers per author, recreating figure 1a in the original paper with different k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./experiments/fig2a.py --n 20 --m 40 --curves 10 --curve_pts 50 --clusters 25 --components 2 --k 5 --df ./Data/data_source_file_experiments.pickle --ff ./experiments/fig2a_k5.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "Perform experiment 2b with recommending k=5 papers per author, recreating figure 1b in the original paper with different k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./experiments/fig2b.py --n 20 --m 40 --curves 10 --curve_pts 50 --beta 0.9 --k 5 --df ./Data/data_source_file_experiments.pickle --ff ./experiments/fig2b_k5.png"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "badd7e1e2f2e7099d7afad4acc4e22811ff33fe7e120d36330385ce27cb21e4b"
  },
  "kernelspec": {
   "display_name": "Python 3.12.8 64-bit ('FACT2025': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
